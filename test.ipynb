{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa39082-8685-4f41-ab2b-1fbd81a20cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 131187/131187 [00:00<00:00, 142632.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL dataset saved to /workspace/dataset/test_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_jsonl_from_csv(csv_file, output_jsonl_file, image_dir):\n",
    "    \"\"\"\n",
    "    Function to create a JSONL file from a CSV where the question is based on the entity_name.\n",
    "    \"\"\"\n",
    "    jsonl_entries = []\n",
    "\n",
    "    # Open the CSV file\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        # Iterate over each row in the CSV with tqdm for progress bar\n",
    "        for idx, row in tqdm(enumerate(reader), total=sum(1 for _ in open(csv_file))-1, desc=\"Processing data\"):\n",
    "            # Get the local image path\n",
    "            image_filename = os.path.basename(row[\"image_link\"])\n",
    "            local_image_path = os.path.join(image_dir, image_filename)\n",
    "\n",
    "            # The question is simply the entity_name\n",
    "            question = f\"{row['entity_name']}\"\n",
    "\n",
    "            entry = {\n",
    "                \"id\": idx,\n",
    "                \"image\": local_image_path,\n",
    "                \"conversations\": [\n",
    "                    {\n",
    "                        \"from\": \"human\",\n",
    "                        \"value\": f\"<image>\\n{question}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            jsonl_entries.append(entry)\n",
    "\n",
    "    # Write the JSONL data to a file\n",
    "    with open(output_jsonl_file, 'w') as f:\n",
    "        for entry in jsonl_entries:\n",
    "            json.dump(entry, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "    print(f\"JSONL dataset saved to {output_jsonl_file}\")\n",
    "\n",
    "# Example usage:\n",
    "csv_file = '/workspace/dataset/test.csv'  # Path to your test CSV file\n",
    "image_dir = '/workspace/dataset/test/'  # Directory where your images are stored\n",
    "output_jsonl_file = '/workspace/dataset/test_data.jsonl'  # Path to save the output JSONL file\n",
    "\n",
    "# Create the JSONL dataset\n",
    "create_jsonl_from_csv(csv_file, output_jsonl_file, image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2317387-e0b4-44cc-bd78-35945090867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9791684-4946-4f0b-809a-dfd462d01b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'maximum_weight_recommendation': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre',\n",
    "        'cubic foot',\n",
    "        'cubic inch',\n",
    "        'cup',\n",
    "        'decilitre',\n",
    "        'fluid ounce',\n",
    "        'gallon',\n",
    "        'imperial gallon',\n",
    "        'litre',\n",
    "        'microlitre',\n",
    "        'millilitre',\n",
    "        'pint',\n",
    "        'quart'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0471e251-1d46-45dc-b402-93418cf5d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(max_new_tokens=1024, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea93a5e-44c6-41f0-92c9-7eb95316f600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test data:   1%|          | 1000/131187 [00:00<00:00, 216536.09it/s]\n",
      "Running inference:   4%|▎         | 37/1000 [02:36<1:14:16,  4.63s/it]"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Function to load the test.jsonl file and extract image paths and questions\n",
    "# def load_test_data_from_jsonl(jsonl_file):\n",
    "#     image_files = []\n",
    "#     questions_list = []\n",
    "#     indices = []\n",
    "\n",
    "#     with open(jsonl_file, 'r') as f:\n",
    "#         for line in tqdm(f, desc=\"Loading test data\", total=sum(1 for _ in open(jsonl_file))):\n",
    "#             data = json.loads(line)\n",
    "#             image_path = data['image']  # Extract the image file path\n",
    "#             image_files.append(image_path)  # Add image file path to the list\n",
    "\n",
    "#             conversation = data['conversations']\n",
    "#             for item in conversation:\n",
    "#                 if item['from'] == 'human':  # Extract the question from the human input\n",
    "#                     key = item['value'].split('\\n')[1]  # Extract entity name from the question\n",
    "\n",
    "#                     # Now use this key to get the corresponding units from entity_unit_map\n",
    "#                     item_weight_units = ', '.join(entity_unit_map[key])\n",
    "\n",
    "#                     # Modify the question string by replacing the placeholder with the actual units\n",
    "#                     question = f' in one of the following ({item_weight_units}) unit'\n",
    "#                     questions_list.append('What is ' + item['value'] + ' of the item' + question)\n",
    "\n",
    "#                     indices.append(data['id'])  # Add the index from the test data\n",
    "                    \n",
    "#     return image_files, questions_list, indices\n",
    "\n",
    "def load_test_data_from_jsonl(jsonl_file, max_lines=1000):\n",
    "    image_files = []\n",
    "    questions_list = []\n",
    "    indices = []\n",
    "\n",
    "    with open(jsonl_file, 'r') as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=\"Loading test data\", total=sum(1 for _ in open(jsonl_file)))):\n",
    "            if i >= max_lines:  # Stop after processing max_lines\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            image_path = data['image']  # Extract the image file path\n",
    "            image_files.append(image_path)  # Add image file path to the list\n",
    "\n",
    "            conversation = data['conversations']\n",
    "            for item in conversation:\n",
    "                if item['from'] == 'human':  # Extract the question from the human input\n",
    "                    key = item['value'].split('\\n')[1]  # Extract entity name from the question\n",
    "\n",
    "                    # Use this key to get the corresponding units from entity_unit_map\n",
    "                    item_weight_units = ', '.join(entity_unit_map[key])\n",
    "\n",
    "                    # Modify the question string by replacing the placeholder with the actual units\n",
    "                    question = f' in one of the following ({item_weight_units}) unit'\n",
    "                    questions_list.append('What is ' + item['value'] + ' of the item' + question)\n",
    "\n",
    "                    indices.append(data['id'])  # Add the index from the test data\n",
    "\n",
    "    return image_files, questions_list, indices\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def convert_and_append_unit(text):\n",
    "    match = re.search(r'\\d+(\\.\\d+)?', text)  # Search for a number (with optional decimal)\n",
    "    if match:\n",
    "        number = float(match.group())  # Convert the matched number to float\n",
    "        unit = text.split()[-1]  # Extract the unit (last part of the string)\n",
    "        return f\"{number} {unit}\"\n",
    "    else:\n",
    "        return \"\"  # Return an empty string if no number is found\n",
    "\n",
    "\n",
    "# Function to run inference and make predictions on test data\n",
    "def run_inference_on_test_data(model, tokenizer, image_files, questions):\n",
    "    predictions = []\n",
    "\n",
    "    for image_file, question in tqdm(zip(image_files, questions), desc=\"Running inference\", total=len(image_files)):\n",
    "        pixel_values = load_image(image_file, max_num=12).to(torch.bfloat16).cuda()\n",
    "        responses = model.batch_chat(tokenizer, pixel_values, num_patches_list=[pixel_values.size(0)], generation_config=generation_config, questions=[question])\n",
    "\n",
    "        predicted_weight = responses[0]  # Assuming a single response per image\n",
    "        predictions.append(convert_and_append_unit(predicted_weight))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Function to save predictions to a CSV file in the required format\n",
    "def save_predictions_to_csv(predictions, indices, output_csv_file):\n",
    "    with open(output_csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['index', 'prediction'])  # Write the header\n",
    "\n",
    "        for idx, prediction in tqdm(zip(indices, predictions), desc=\"Saving predictions\", total=len(predictions)):\n",
    "            writer.writerow([idx, prediction])  # Write index and prediction\n",
    "\n",
    "\n",
    "# Load test data (image paths, questions, and indices) from JSONL file\n",
    "image_files, questions_list, indices = load_test_data_from_jsonl('/workspace/dataset/test_data.jsonl')\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    num_layers = {\n",
    "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
    "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map\n",
    "\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "# Load model and tokenizer\n",
    "# path = '/workspace/work_dirs/internvl_chat_v2_0/internvl2_2b_internlm2_1_8b_dynamic_res_2nd_finetune_lora_amazon_merge'\n",
    "# device_map = split_model('InternVL2-8B')\n",
    "path = 'OpenGVLab/InternVL2-2B'\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "# Run inference on the test data\n",
    "predictions = run_inference_on_test_data(model, tokenizer, image_files, questions_list)\n",
    "\n",
    "# Save the predictions to a CSV file in the required format\n",
    "output_csv_file = '/workspace/dataset/test_predictions.csv'\n",
    "save_predictions_to_csv(predictions, indices, output_csv_file)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38c5e75-1e41-4440-a607-61ad92161d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7c0bb-778c-45b0-98ca-b1fd901972f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf837f17-8c16-44b7-b09d-4e13a03ed51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbc0f2-d6d1-4936-a1cb-8f777420eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test data: 100%|██████████| 1000/1000 [00:00<00:00, 226413.17it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f158b5244fa64cec883512a9b7425ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1420d7bf19f4fa28be43ef8f755baab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_internvl_chat.py:   0%|          | 0.00/3.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2dd670877d42558565ad6d5365c709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_internlm2.py:   0%|          | 0.00/7.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-2B:\n",
      "- configuration_internlm2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77190d7544ba419a810866358735b808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_intern_vit.py:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-2B:\n",
      "- configuration_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-2B:\n",
      "- configuration_internvl_chat.py\n",
      "- configuration_internlm2.py\n",
      "- configuration_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfcbada1e2b47bbb819019d3a2e3914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_internvl_chat.py:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2b45c1f8654785aa22202255b8b73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_intern_vit.py:   0%|          | 0.00/18.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-2B:\n",
      "- modeling_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020ac668c3264cadab0d1a5dbeddd19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_internlm2.py:   0%|          | 0.00/61.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-2B:\n",
      "- modeling_internlm2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8635020794b4e98ae3bd2726675444e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conversation.py:   0%|          | 0.00/15.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-2B:\n",
      "- conversation.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-2B:\n",
      "- modeling_internvl_chat.py\n",
      "- modeling_intern_vit.py\n",
      "- modeling_internlm2.py\n",
      "- conversation.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cbd15006b8408080373817fab94ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d16293eb55b40ef9725c84adef0fd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 24 layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f690409f3716411ea047276084534aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04faffe8fa8545f1852762508cd97723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_internlm2.py:   0%|          | 0.00/8.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-2B:\n",
      "- tokenization_internlm2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eabff6ff4314ff88d0829d143ad9254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64953c3ea659422a8714dbd898592680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6078a5a03ba1409695e6aaf8e62f58ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   2%|▏         | 1/63 [01:54<1:58:48, 114.97s/it]"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer, GenerationConfig\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the entity_unit_map\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon',\n",
    "                    'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, jsonl_file, transform, entity_unit_map, max_lines=1000):\n",
    "        self.image_paths = []\n",
    "        self.questions = []\n",
    "        self.indices = []\n",
    "        self.transform = transform\n",
    "        self.entity_unit_map = entity_unit_map\n",
    "\n",
    "        with open(jsonl_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if max_lines:\n",
    "                lines = lines[:max_lines]\n",
    "            for line in tqdm(lines, desc=\"Loading test data\"):\n",
    "                data = json.loads(line)\n",
    "                image_path = data['image']  # Extract the image file path\n",
    "                self.image_paths.append(image_path)  # Add image file path to the list\n",
    "\n",
    "                conversation = data['conversations']\n",
    "                for item in conversation:\n",
    "                    if item['from'] == 'human':  # Extract the question from the human input\n",
    "                        key = item['value'].split('\\n')[1]  # Extract entity name from the question\n",
    "\n",
    "                        # Use this key to get the corresponding units from entity_unit_map\n",
    "                        item_weight_units = ', '.join(self.entity_unit_map[key])\n",
    "\n",
    "                        # Modify the question string by replacing the placeholder with the actual units\n",
    "                        question = f\"What is {key} of the item in one of the following units ({item_weight_units})?\"\n",
    "                        self.questions.append(question)\n",
    "\n",
    "                        self.indices.append(data['id'])  # Add the index from the test data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and preprocess the image\n",
    "        image_file = self.image_paths[idx]\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "\n",
    "        # Process image into patches\n",
    "        images = dynamic_preprocess(\n",
    "            image,\n",
    "            image_size=448,\n",
    "            use_thumbnail=True,\n",
    "            max_num=12\n",
    "        )\n",
    "        pixel_values = [self.transform(img) for img in images]\n",
    "        pixel_values = torch.stack(pixel_values)  # Shape: [num_patches, channels, height, width]\n",
    "\n",
    "        # Get the question and index\n",
    "        question = self.questions[idx]\n",
    "        index = self.indices[idx]\n",
    "\n",
    "        return pixel_values, question, index\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values_list, questions, indices = zip(*batch)\n",
    "    # Combine pixel values into a list\n",
    "    pixel_values = list(pixel_values_list)\n",
    "    questions = list(questions)\n",
    "    indices = list(indices)\n",
    "    return pixel_values, questions, indices\n",
    "\n",
    "def convert_and_append_unit(text):\n",
    "    match = re.search(r'\\d+(\\.\\d+)?', text)  # Search for a number (with optional decimal)\n",
    "    if match:\n",
    "        number = float(match.group())  # Convert the matched number to float\n",
    "        unit = text.split()[-1]  # Extract the unit (last part of the string)\n",
    "        return f\"{number} {unit}\"\n",
    "    else:\n",
    "        return \"\"  # Return an empty string if no number is found\n",
    "\n",
    "def run_inference_on_test_data(model, tokenizer, dataloader, device):\n",
    "    predictions = []\n",
    "    indices_list = []\n",
    "\n",
    "    model.eval()\n",
    "    for pixel_values_list, questions, indices in tqdm(dataloader, desc=\"Running inference\"):\n",
    "        # Move inputs to the appropriate device and flatten pixel values\n",
    "        pixel_values = [pv.to(device, dtype=torch.bfloat16) for pv in pixel_values_list]\n",
    "        num_patches_list = [pv.size(0) for pv in pixel_values]  # Number of patches per image\n",
    "\n",
    "        # Concatenate pixel_values along the batch dimension\n",
    "        pixel_values = torch.cat(pixel_values, dim=0)  # Shape: [total_patches, channels, height, width]\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            responses = model.batch_chat(\n",
    "                tokenizer,\n",
    "                pixel_values,\n",
    "                num_patches_list=num_patches_list,\n",
    "                generation_config=generation_config,\n",
    "                questions=questions\n",
    "            )\n",
    "\n",
    "        for response in responses:\n",
    "            predicted_weight = convert_and_append_unit(response)\n",
    "            predictions.append(predicted_weight)\n",
    "\n",
    "        indices_list.extend(indices)\n",
    "\n",
    "    return predictions, indices_list\n",
    "\n",
    "def save_predictions_to_csv(predictions, indices, output_csv_file):\n",
    "    with open(output_csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['index', 'prediction'])  # Write the header\n",
    "\n",
    "        for idx, prediction in tqdm(zip(indices, predictions), desc=\"Saving predictions\", total=len(predictions)):\n",
    "            writer.writerow([idx, prediction])  # Write index and prediction\n",
    "\n",
    "def split_model(model_name, num_layers):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu_list = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu_list[0] = math.ceil(num_layers_per_gpu_list[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu_list):\n",
    "        for _ in range(num_layer):\n",
    "            if layer_cnt < num_layers:\n",
    "                device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "                layer_cnt += 1\n",
    "\n",
    "    # Assign other components to GPU 0\n",
    "    components_on_gpu0 = [\n",
    "        'vision_model', 'mlp1', 'language_model.model.tok_embeddings',\n",
    "        'language_model.model.embed_tokens', 'language_model.output',\n",
    "        'language_model.model.norm', 'language_model.lm_head',\n",
    "        f'language_model.model.layers.{num_layers - 1}'\n",
    "    ]\n",
    "    for component in components_on_gpu0:\n",
    "        device_map[component] = 0\n",
    "\n",
    "    return device_map\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Build the image transform\n",
    "    transform = build_transform(input_size=448)\n",
    "\n",
    "    # Create the dataset\n",
    "    test_dataset = TestDataset(\n",
    "        jsonl_file='/workspace/dataset/test_data.jsonl',\n",
    "        transform=transform,\n",
    "        entity_unit_map=entity_unit_map,\n",
    "        max_lines=1000  # Set to an integer if you want to limit the number of samples\n",
    "    )\n",
    "\n",
    "    # Create the DataLoader with the custom collate function\n",
    "    batch_size = 16  # Adjust based on your GPU memory\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    # path = '/workspace/work_dirs/internvl_chat_v2_0/internvl2_2b_internlm2_1_8b_dynamic_res_2nd_finetune_lora_amazon_merge'\n",
    "    path = 'OpenGVLab/InternVL2-2B'\n",
    "    model_name = 'InternVL2-2B'  # Adjust according to your model\n",
    "\n",
    "    # Load the model temporarily to get the number of layers\n",
    "    model_temp = AutoModel.from_pretrained(\n",
    "        path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    actual_num_layers = len(model_temp.language_model.model.layers)\n",
    "    print(f\"The model has {actual_num_layers} layers.\")\n",
    "    del model_temp\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    device_map = split_model(model_name, actual_num_layers)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map=device_map\n",
    "    ).eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    # Adjust the code in modeling_internvl_chat.py if necessary\n",
    "    # For example, change generation_config['eos_token_id'] to generation_config.eos_token_id\n",
    "\n",
    "    # Determine the device for the vision model (assumed to be on GPU 0)\n",
    "    device = torch.device(f'cuda:{device_map[\"vision_model\"]}')\n",
    "\n",
    "    # Run inference on the test data\n",
    "    predictions, indices = run_inference_on_test_data(model, tokenizer, test_loader, device)\n",
    "\n",
    "    # Save the predictions to a CSV file in the required format\n",
    "    output_csv_file = '/workspace/dataset/test_predictions.csv'\n",
    "    save_predictions_to_csv(predictions, indices, output_csv_file)\n",
    "\n",
    "    print(f\"Predictions saved to {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7dccb3-fa1c-44f1-a529-f5b17cc5c2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
